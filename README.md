# ğŸ§  Prompt Evaluator App (Local LLM with Ollama + Streamlit)

A lightweight Streamlit app to evaluate and compare AI prompts using a locally hosted LLM via [Ollama](https://ollama.com/). No OpenAI key or cloud dependency required.

---

## ğŸš€ Features

- ğŸ§ª Evaluate custom prompts against a local LLM
- ğŸ”’ Runs fully offline using Ollama
- ğŸ–¥ï¸ Streamlit-powered clean UI
- âš¡ Supports small models like `tinyllama` for low-memory systems

---

## ğŸ› ï¸ Requirements

- Python 3.10+
- Git
- [Ollama](https://ollama.com/download) installed and running
- A supported Ollama model installed (e.g., `tinyllama`, `llama3`, etc.)

---

## ğŸ“¦ Installation

1. **Clone the repository**:

   ```bash
   git clone https://github.com/rohitpote-15/prompt-evaluator-ollama.git
   cd prompt-evaluator-ollama
````

2. **Create a virtual environment** (optional but recommended):

   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**:

   ```bash
   pip install -r requirements.txt
   ```

4. **Start Ollama** (make sure it's running in background):

   ```bash
   ollama run tinyllama
   ```

   Or preload a model:

   ```bash
   ollama pull tinyllama
   ```

---

## â–¶ï¸ Run the App

```bash
streamlit run src/app.py
```

Once running, open your browser at: [http://localhost:8501](http://localhost:8501)

---

## ğŸ“ Project Structure

```
prompt-evaluator-ollama/
â”‚
â”œâ”€â”€ .env.template           # Template for environment variables (no secrets)
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py              # Streamlit app logic
â”‚   â””â”€â”€ prompts.json        # Pre-defined or saved prompts
â””â”€â”€ .gitignore
```

---

## ğŸ§© Customization

* Edit `prompts.json` to include your own prompt templates.
* Switch models in `app.py` by replacing:

  ```python
  Ollama(model="tinyllama")
  ```

  with any other supported local model (e.g., `llama3`, `mistral`, etc.)

---

## ğŸ›‘ Notes

* This app will not run on **Streamlit Cloud** https://prompt-evaluator-ollama.streamlit.app/, but due to its dependency on `localhost:xxxx`.
* Your system must have enough RAM to load the selected LLM model via Ollama.

---

## âœ¨ Author

Rohit Pote

ğŸ’¼ AI/ML Engineer | LLM Architect | Prompt Engineer
ğŸŒ LinkedIn
ğŸ“¬ rohitrajupote97@gmail.com
ğŸš€ [Streamlit App] https://legal-docs-rag-chatbot.streamlit.app/

---
